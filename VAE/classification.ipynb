{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import log_normal, log_normal_mixture\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded array shape: (93686, 36)\n",
      "Columns from OneHotEncoder: ['sub_category_Against Interest of sovereignty or integrity of India'\n",
      " 'sub_category_Business Email CompromiseEmail Takeover'\n",
      " 'sub_category_Cheating by Impersonation'\n",
      " 'sub_category_Cryptocurrency Fraud'\n",
      " 'sub_category_Cyber Bullying  Stalking  Sexting'\n",
      " 'sub_category_Cyber Terrorism'\n",
      " 'sub_category_Damage to computer computer systems etc'\n",
      " 'sub_category_Data Breach/Theft'\n",
      " 'sub_category_DebitCredit Card FraudSim Swap Fraud'\n",
      " 'sub_category_DematDepository Fraud'\n",
      " 'sub_category_Denial of Service (DoS)/Distributed Denial of Service (DDOS) attacks'\n",
      " 'sub_category_EMail Phishing' 'sub_category_EWallet Related Fraud'\n",
      " 'sub_category_Email Hacking' 'sub_category_FakeImpersonating Profile'\n",
      " 'sub_category_Fraud CallVishing' 'sub_category_Hacking/Defacement'\n",
      " 'sub_category_Impersonating Email'\n",
      " 'sub_category_Internet Banking Related Fraud'\n",
      " 'sub_category_Intimidating Email' 'sub_category_Malware Attack'\n",
      " 'sub_category_Online Gambling  Betting' 'sub_category_Online Job Fraud'\n",
      " 'sub_category_Online Matrimonial Fraud' 'sub_category_Online Trafficking'\n",
      " 'sub_category_Other' 'sub_category_Profile Hacking Identity Theft'\n",
      " 'sub_category_Provocative Speech for unlawful acts'\n",
      " 'sub_category_Ransomware' 'sub_category_Ransomware Attack'\n",
      " 'sub_category_SQL Injection'\n",
      " 'sub_category_Tampering with computer source documents'\n",
      " 'sub_category_UPI Related Frauds'\n",
      " 'sub_category_Unauthorised AccessData Breach'\n",
      " 'sub_category_Website DefacementHacking' 'sub_category_nan']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset preparation. Apply for train and test dataset\n",
    "\"\"\"\n",
    "dataset = pd.read_csv('train_processed_data.csv', low_memory=False)\n",
    "\n",
    "# Step 2: Extract unique labels from the 'category' column\n",
    "categories = dataset['sub_category'].unique()\n",
    "\n",
    "# Step 3: Apply one-hot encoding\n",
    "encoder = OneHotEncoder()  # sparse=False returns a dense array\n",
    "one_hot_vectors = encoder.fit_transform(dataset[['sub_category']])\n",
    "\n",
    "# Step 3: Check the shape of the one-hot encoded array and the number of columns\n",
    "print(f\"One-hot encoded array shape: {one_hot_vectors.shape}\")\n",
    "print(f\"Columns from OneHotEncoder: {encoder.get_feature_names_out(['sub_category'])}\")\n",
    "\n",
    "one_hot_vectors = one_hot_vectors.toarray()\n",
    "\n",
    "# Step 3: Store one-hot vectors as lists in a new column in the original DataFrame\n",
    "dataset['one_hot_vectors'] = one_hot_vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>crimeaditionalinfo</th>\n",
       "      <th>crimeaditionalinfo_preprocessed</th>\n",
       "      <th>one_hot_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Cyber Bullying  Stalking  Sexting</td>\n",
       "      <td>I had continue received random calls and abusi...</td>\n",
       "      <td>continu receiv random call abus messag whatsap...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>The above fraudster is continuously messaging ...</td>\n",
       "      <td>fraudster continu messag ask pay money send fa...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>Online Gambling  Betting</td>\n",
       "      <td>He is acting like a police and demanding for m...</td>\n",
       "      <td>act like polic demand money ad section text me...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Online Job Fraud</td>\n",
       "      <td>In apna Job I have applied for job interview f...</td>\n",
       "      <td>apna job appli job interview telecal resourc m...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>I received a call from lady stating that she w...</td>\n",
       "      <td>receiv call ladi state send new phone vivo rec...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93681</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Internet Banking Related Fraud</td>\n",
       "      <td>Identity theft   Smishing SMS Fraud  CreditDeb...</td>\n",
       "      <td>ident theft smish sm fraud creditdebit card fr...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93682</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>EWallet Related Fraud</td>\n",
       "      <td>RECEIVED CALL FROM  NUMBER ASKING ABOUT phone ...</td>\n",
       "      <td>receiv call number ask phone pay cash back off...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93683</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>UPI Related Frauds</td>\n",
       "      <td>Cyber Stalking   Blackmailing   PhoneSMSVOIP C...</td>\n",
       "      <td>cyber stalk blackmail phonesmsvoip call victim...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93684</th>\n",
       "      <td>Online and Social Media Related Crime</td>\n",
       "      <td>Online Matrimonial Fraud</td>\n",
       "      <td>Call karke bola ki aapka lotary laga ha aru AC...</td>\n",
       "      <td>call kark bola ki aapka lotari laga ha aru ac ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93685</th>\n",
       "      <td>Online Financial Fraud</td>\n",
       "      <td>Fraud CallVishing</td>\n",
       "      <td>There is app name koko loan app they send the ...</td>\n",
       "      <td>app name koko loan app send money account unkn...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93686 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    category  \\\n",
       "0      Online and Social Media Related Crime   \n",
       "1                     Online Financial Fraud   \n",
       "2                   Online Gambling  Betting   \n",
       "3      Online and Social Media Related Crime   \n",
       "4                     Online Financial Fraud   \n",
       "...                                      ...   \n",
       "93681                 Online Financial Fraud   \n",
       "93682                 Online Financial Fraud   \n",
       "93683                 Online Financial Fraud   \n",
       "93684  Online and Social Media Related Crime   \n",
       "93685                 Online Financial Fraud   \n",
       "\n",
       "                            sub_category  \\\n",
       "0      Cyber Bullying  Stalking  Sexting   \n",
       "1                      Fraud CallVishing   \n",
       "2               Online Gambling  Betting   \n",
       "3                       Online Job Fraud   \n",
       "4                      Fraud CallVishing   \n",
       "...                                  ...   \n",
       "93681     Internet Banking Related Fraud   \n",
       "93682              EWallet Related Fraud   \n",
       "93683                 UPI Related Frauds   \n",
       "93684           Online Matrimonial Fraud   \n",
       "93685                  Fraud CallVishing   \n",
       "\n",
       "                                      crimeaditionalinfo  \\\n",
       "0      I had continue received random calls and abusi...   \n",
       "1      The above fraudster is continuously messaging ...   \n",
       "2      He is acting like a police and demanding for m...   \n",
       "3      In apna Job I have applied for job interview f...   \n",
       "4      I received a call from lady stating that she w...   \n",
       "...                                                  ...   \n",
       "93681  Identity theft   Smishing SMS Fraud  CreditDeb...   \n",
       "93682  RECEIVED CALL FROM  NUMBER ASKING ABOUT phone ...   \n",
       "93683  Cyber Stalking   Blackmailing   PhoneSMSVOIP C...   \n",
       "93684  Call karke bola ki aapka lotary laga ha aru AC...   \n",
       "93685  There is app name koko loan app they send the ...   \n",
       "\n",
       "                         crimeaditionalinfo_preprocessed  \\\n",
       "0      continu receiv random call abus messag whatsap...   \n",
       "1      fraudster continu messag ask pay money send fa...   \n",
       "2      act like polic demand money ad section text me...   \n",
       "3      apna job appli job interview telecal resourc m...   \n",
       "4      receiv call ladi state send new phone vivo rec...   \n",
       "...                                                  ...   \n",
       "93681  ident theft smish sm fraud creditdebit card fr...   \n",
       "93682  receiv call number ask phone pay cash back off...   \n",
       "93683  cyber stalk blackmail phonesmsvoip call victim...   \n",
       "93684  call kark bola ki aapka lotari laga ha aru ac ...   \n",
       "93685  app name koko loan app send money account unkn...   \n",
       "\n",
       "                                         one_hot_vectors  \n",
       "0      [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                  ...  \n",
       "93681  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "93682  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "93683  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "93684  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "93685  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[93686 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "num_labels=len(categories)\n",
    "\n",
    "# pytorch will be run on cpu\n",
    "seed=900\n",
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# General Model variables\n",
    "args.seed=seed\n",
    "args.lr=1e-3\n",
    "args.lr_decay_ratio=0.9\n",
    "args.lr_decay_times=4\n",
    "args.nll_coeff=2\n",
    "args.l2_coeff=1\n",
    "args.c_coeff=0\n",
    "args.class_weights=1  # Find the class weights depending on count\n",
    "args.current_step=0 # For Plotting the loss and other variables w.r.t iterations\n",
    "\n",
    "# Set Dataloader parameters\n",
    "args.BATCH_SIZE = 64\n",
    "args.NUM_WORKERS = 8\n",
    "args.max_epoch=300\n",
    "\n",
    "# scheduler variables\n",
    "args.eta_min=2e-4\n",
    "args.T_mult=2\n",
    "args.T0=50\n",
    "args.retrain=input(\"Enter True or False if you want to retrain\")=='True'\n",
    "\n",
    "# VAE model variables\n",
    "args.latent_dim=128\n",
    "args.drop=0.1 # dropout value\n",
    "args.feature_dim=256 # Output dimension from flattened ResNet\n",
    "args.emb_size=128 # What is emb_size ?\n",
    "args.label_dim=num_labels # Label Dimension\n",
    "args.param_setting = \"lr-{}_lr-decay_{:.2f}_lr-times_{:.1f}_nll-{:.2f}_l2-{:.2f}_c-{:.2f}\".format(args.lr, args.lr_decay_ratio, args.lr_decay_times, args.nll_coeff, args.l2_coeff, args.c_coeff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Bert tokenizer or any other tokenizer deemed suitable\n",
    "# Test the tokenizer\n",
    "test_text = \"We are testing BERT tokenizer.\"\n",
    "# generate encodings\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "encodings = tokenizer.encode_plus(test_text, \n",
    "                                  add_special_tokens = True,\n",
    "                                  max_length = 50,\n",
    "                                  truncation = True,\n",
    "                                  padding = \"max_length\", \n",
    "                                  return_attention_mask = True, \n",
    "                                  return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom Data Loader\n",
    "\"\"\"\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_column):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.title = list(df['crimeaditionalinfo_preprocessed'])\n",
    "        self.targets = self.df[target_column]\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index]),\n",
    "            'title': title\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(dataset, tokenizer, MAX_LEN, 'one_hot_vectors')\n",
    "\n",
    "# Change the test and val dataset \n",
    "valid_dataset = CustomDataset(dataset, tokenizer, MAX_LEN, 'one_hot_vectors')\n",
    "test_dataset = CustomDataset(dataset, tokenizer, MAX_LEN, 'one_hot_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Model here\n",
    "\"\"\"\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear = torch.nn.Linear(768, 256)  # Take care to change outputs of Bert class as what would be inputs of VAE\n",
    "    \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VAE model\n",
    "\"\"\"\n",
    "\n",
    "class base_class(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(base_class, self).__init__()\n",
    "        self.args=args\n",
    "        self.model=BERTClass() #weights='ResNet101_Weights.IMAGENET1K_V1'\n",
    "        #self.model.fc=nn.Flatten() # Flatten the last layer\n",
    "        print(self.model)\n",
    "    def forward(self,ids, mask, token_type_ids):\n",
    "        return self.model(ids, mask, token_type_ids)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VAE, self).__init__()\n",
    "        self.args = args\n",
    "        self.dropout = nn.Dropout(p=args.drop)\n",
    "\n",
    "        self.base_model=base_class(args)\n",
    "        \n",
    "        \"\"\"Feature encoder\"\"\"\n",
    "        self.fx = nn.Sequential(\n",
    "            nn.Linear(args.feature_dim, 256), # Set args.feature dim according to flatten shape. By default it is 2048\n",
    "            nn.ReLU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(256, 512,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(512, 512,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(512, 256,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout\n",
    "        )\n",
    "        self.fx_mu = nn.Linear(256, args.latent_dim,bias=True)\n",
    "        self.fx_logvar = nn.Linear(256, args.latent_dim,bias=True)\n",
    "\n",
    "        \"\"\"Label encoder\"\"\"\n",
    "        self.label_lookup = nn.Linear(args.label_dim, args.emb_size)\n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Linear(args.emb_size, 512,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout,\n",
    "            nn.Linear(512, 256,bias=True),\n",
    "            nn.ReLU(),\n",
    "            self.dropout\n",
    "        )\n",
    "        self.fe_mu = nn.Linear(256, args.latent_dim,bias=True)\n",
    "        self.fe_logvar = nn.Linear(256, args.latent_dim,bias=True)\n",
    "\n",
    "        \"\"\"Decoder\"\"\"\n",
    "        self.fd = nn.Sequential(\n",
    "            nn.Linear(args.feature_dim + args.latent_dim, 512,bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, args.emb_size,bias=True),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        # Adaptive weight loss\n",
    "        import torch.nn.init as init\n",
    "        # Define the linear layer\n",
    "        self.linear_layer_weight = nn.Sequential(\n",
    "            nn.Linear(256, 128,bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_labels,bias=True), #Changed. Before working prop\n",
    "            nn.Softmax(),#Changed. Before working prop\n",
    "        )\n",
    "\n",
    "    def label_encode(self, x):\n",
    "        h0 = self.dropout(F.relu(self.label_lookup(x)))\n",
    "        h = self.fe(h0)\n",
    "        mu = self.fe_mu(h)\n",
    "        logvar = self.fe_logvar(h)\n",
    "        fe_output = {\n",
    "            'fe_mu': mu,\n",
    "            'fe_logvar': logvar\n",
    "        }\n",
    "        return fe_output\n",
    "\n",
    "    def feat_encode(self, x):\n",
    "        #print(x.shape)\n",
    "        h = self.fx(x)\n",
    "        mu = self.fx_mu(h)\n",
    "        logvar = self.fx_logvar(h)\n",
    "        fx_output = {\n",
    "            'fx_mu': mu,\n",
    "            'fx_logvar': logvar\n",
    "        }\n",
    "        return fx_output\n",
    "\n",
    "    def decode(self, z):\n",
    "        d = self.fd(z)\n",
    "        d = F.normalize(d, dim=1)\n",
    "        return d\n",
    "\n",
    "    def label_forward(self, x, feat):\n",
    "        n_label = x.shape[1]\n",
    "        all_labels = torch.eye(n_label).to(device)\n",
    "        fe_output = self.label_encode(all_labels)\n",
    "        mu = fe_output['fe_mu']\n",
    "        \n",
    "        z = torch.matmul(x, mu) / x.sum(1, keepdim=True)\n",
    "        #print(feat.shape,z.shape)\n",
    "        label_emb = self.decode(torch.cat((feat, z), 1))\n",
    "\n",
    "        fe_output['label_emb'] = label_emb\n",
    "        return fe_output\n",
    "\n",
    "    def adaptive(self,x):\n",
    "        # Adaptive weight\n",
    "        x=self.linear_layer_weight(x)\n",
    "        return x\n",
    "    \n",
    "    def feat_forward(self, x):\n",
    "        fx_output = self.feat_encode(x)\n",
    "        mu = fx_output['fx_mu']\n",
    "        logvar = fx_output['fx_logvar']\n",
    "\n",
    "        if not self.training:\n",
    "            z = mu\n",
    "            z2 = mu\n",
    "        else:\n",
    "            z = reparameterize(mu, logvar)\n",
    "            z2 = reparameterize(mu, logvar)\n",
    "        feat_emb = self.decode(torch.cat((x, z), 1))\n",
    "        feat_emb2 = self.decode(torch.cat((x, z2), 1))\n",
    "        fx_output['feat_emb'] = feat_emb\n",
    "        fx_output['feat_emb2'] = feat_emb2\n",
    "        return fx_output\n",
    "\n",
    "    def forward(self, label, ids, mask, token_type_ids):\n",
    "        # Apply resnet model to get feature embeddings\n",
    "        feature=self.base_model(ids, mask, token_type_ids)\n",
    "        #w_1=self.adaptive(feature) # Changed\n",
    "        fe_output = self.label_forward(label, feature)\n",
    "        label_emb = fe_output['label_emb']\n",
    "        fx_output = self.feat_forward(feature)\n",
    "        feat_emb, feat_emb2 = fx_output['feat_emb'], fx_output['feat_emb2']\n",
    "\n",
    "        embs = self.label_lookup.weight\n",
    "        label_out = torch.matmul(label_emb, embs)\n",
    "        feat_out = torch.matmul(feat_emb, embs)\n",
    "        feat_out2 = torch.matmul(feat_emb2, embs)\n",
    "        \n",
    "        fe_output.update(fx_output)\n",
    "        output = fe_output\n",
    "        output['embs'] = embs\n",
    "        output['label_out'] = label_out\n",
    "        output['feat_out'] = feat_out\n",
    "        output['feat_out2'] = feat_out2\n",
    "        output['feat'] = feature\n",
    "        #output['weight_loss']=w_1 # Changed\n",
    "        #print(\"W1\",w_1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps*std\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_loss(input_label, output, args=None):\n",
    "    fe_out, fe_mu, fe_logvar, label_emb = \\\n",
    "        output['label_out'], output['fe_mu'], output['fe_logvar'], output['label_emb']\n",
    "    fx_out, fx_mu, fx_logvar, feat_emb = \\\n",
    "        output['feat_out'], output['fx_mu'], output['fx_logvar'], output['feat_emb']\n",
    "    fx_out2 = output['feat_out2']\n",
    "    embs = output['embs']\n",
    "\n",
    "    fx_sample = reparameterize(fx_mu, fx_logvar)\n",
    "    fx_var = torch.exp(fx_logvar)\n",
    "    fe_var = torch.exp(fe_logvar)\n",
    "    kl_loss = (log_normal(fx_sample, fx_mu, fx_var) - \\\n",
    "        log_normal_mixture(fx_sample, fe_mu, fe_var, input_label)).mean()\n",
    "\n",
    "    #pred_e = torch.sigmoid(fe_out)\n",
    "    #pred_x = torch.sigmoid(fx_out)\n",
    "    #pred_x2 = torch.sigmoid(fx_out2)\n",
    "\n",
    "    # Use softmax for multi-class classification\n",
    "    pred_e = F.softmax(fe_out, dim=1)\n",
    "    pred_x = F.softmax(fx_out, dim=1)\n",
    "    pred_x2 = F.softmax(fx_out2, dim=1)\n",
    "\n",
    "     # Define cross-entropy loss for multi-class classification\n",
    "    def compute_CE_loss(pred):\n",
    "        return F.cross_entropy(pred, input_label)\n",
    "\n",
    "    def compute_BCE_and_RL_loss(E):\n",
    "        #compute negative log likelihood (BCE loss) for each sample point\n",
    "        sample_nll = -(\n",
    "            #torch.mul((torch.log(E) * input_label + torch.log(1 - E) * (1 - input_label)),output['weight_loss']) # Changed here to add adaptive weights\n",
    "            (torch.log(E) * input_label + torch.log(1 - E) * (1 - input_label))\n",
    "        )\n",
    "        logprob = -torch.sum(sample_nll, dim=2)\n",
    "\n",
    "        #the following computation is designed to avoid the float overflow (log_sum_exp trick)\n",
    "        maxlogprob = torch.max(logprob, dim=0)[0]\n",
    "        Eprob = torch.mean(torch.exp(logprob - maxlogprob), axis=0)\n",
    "        nll_loss = torch.mean(-torch.log(Eprob) - maxlogprob)\n",
    "        return nll_loss\n",
    "\n",
    "    def supconloss(label_emb, feat_emb, embs, temp=1.0):\n",
    "        features = torch.cat((label_emb, feat_emb))\n",
    "        labels = torch.cat((input_label, input_label)).float()\n",
    "        n_label = labels.shape[1]\n",
    "        emb_labels = torch.eye(n_label).to(device)\n",
    "        mask = torch.matmul(labels, emb_labels)\n",
    "\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features, embs),\n",
    "            temp)\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        exp_logits = torch.exp(logits)\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "        loss = -mean_log_prob_pos\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    nll_loss = compute_CE_loss(pred_e) #compute_BCE_and_RL_loss(pred_e.unsqueeze(0))\n",
    "    nll_loss_x = compute_CE_loss(pred_x) #compute_BCE_and_RL_loss(pred_x.unsqueeze(0))\n",
    "    nll_loss_x2 = compute_CE_loss(pred_x2) #compute_BCE_and_RL_loss(pred_x2.unsqueeze(0))\n",
    "    sum_nll_loss = nll_loss + nll_loss_x + nll_loss_x2\n",
    "    cpc_loss = supconloss(label_emb, feat_emb, embs)\n",
    "    total_loss = sum_nll_loss * args.nll_coeff + kl_loss * 6. + cpc_loss\n",
    "    return total_loss, nll_loss, nll_loss_x, 0., 0., kl_loss, cpc_loss, pred_e, pred_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VAE(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "# Separate out the model parameters if required\n",
    "args.optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=5e-4)\n",
    "args.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(args.optimizer, eta_min=args.eta_min, T_0=args.T0, T_mult=args.T_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load pretrained model here\n",
    "#model=torch.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the losses\n",
    "\n",
    "# smooth means average. Every batch has a mean loss value w.r.t. different losses\n",
    "smooth_nll_loss=0.0 # label encoder decoder cross entropy loss\n",
    "smooth_nll_loss_x=0.0 # feature encoder decoder cross entropy loss\n",
    "smooth_c_loss = 0.0 # label encoder decoder ranking loss\n",
    "smooth_c_loss_x=0.0 # feature encoder decoder ranking loss\n",
    "smooth_kl_loss = 0.0 # kl divergence\n",
    "smooth_total_loss=0.0 # total loss\n",
    "smooth_macro_f1 = 0.0 # macro_f1 score\n",
    "smooth_micro_f1 = 0.0 # micro_f1 score\n",
    "\n",
    "best_loss = 1e10\n",
    "best_iter = 0\n",
    "best_macro_f1 = 0.0 # best macro f1 for ckpt selection in validation\n",
    "best_micro_f1 = 0.0 # best micro f1 for ckpt selection in validation\n",
    "best_acc = 0.0 # best subset acc for ckpt selction in validation\n",
    "\n",
    "temp_label=[]\n",
    "temp_pred_x=[]\n",
    "\n",
    "\n",
    "best_test_metrics = None\n",
    "\n",
    "loss_list = [\"nll_loss\",\n",
    "             \"total_loss\",\n",
    "             \"acc\",\n",
    "             \"hamm_acc\",\n",
    "             \"macro_f1\",\n",
    "             \"micro_f1\",\n",
    "             'auc',\n",
    "             'roc']\n",
    "\n",
    "train_losses = {i:[] for i in loss_list}\n",
    "test_losses = {i:[] for i in loss_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Function\n",
    "import tqdm as tqdm\n",
    "import evals\n",
    "\n",
    "def train(model,train_loader, args):\n",
    "    print(\"Training Started\")\n",
    "    counter=0\n",
    "    model=model.to(device)\n",
    "    # smooth means average. Every batch has a mean loss value w.r.t. different losses\n",
    "    smooth_nll_loss=0.0 # label encoder decoder cross entropy loss\n",
    "    smooth_nll_loss_x=0.0 # feature encoder decoder cross entropy loss\n",
    "    smooth_c_loss = 0.0 # label encoder decoder ranking loss\n",
    "    smooth_c_loss_x=0.0 # feature encoder decoder ranking loss\n",
    "    smooth_kl_loss = 0.0 # kl divergence\n",
    "    smooth_total_loss=0.0 # total loss\n",
    "    smooth_macro_f1 = 0.0 # macro_f1 score\n",
    "    smooth_micro_f1 = 0.0 # micro_f1 score\n",
    "    \n",
    "    for _,data in tqdm(enumerate(train_loader),desc='Training'):\n",
    "        #print(\"Entered Loop\")\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "        # print(targets)\n",
    "        args.optimizer.zero_grad()\n",
    "        output = model(targets,ids, mask, token_type_ids)\n",
    "        #print(\"Output Done!\")\n",
    "        total_loss, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, cpc_loss, _, pred_x = \\\n",
    "                    compute_loss(targets, output, args)\n",
    "        total_loss=total_loss\n",
    "        total_loss.backward()\n",
    "        args.optimizer.step()\n",
    "\n",
    "        train_metrics = evals.compute_metrics(pred_x.cpu().data.numpy(), targets.cpu().data.numpy(), 0.5, all_metrics=False)\n",
    "        macro_f1, micro_f1 = train_metrics['maF1'], train_metrics['miF1']\n",
    "       \n",
    "        smooth_nll_loss += nll_loss\n",
    "        smooth_nll_loss_x += nll_loss_x\n",
    "        smooth_c_loss += c_loss\n",
    "        smooth_c_loss_x += c_loss_x\n",
    "        smooth_kl_loss += kl_loss\n",
    "        smooth_total_loss += total_loss\n",
    "        smooth_macro_f1 += macro_f1\n",
    "        smooth_micro_f1 += micro_f1\n",
    "\n",
    "        counter+=1\n",
    "        #print(\"Train Func\",counter)\n",
    "        #del x,targets,outputs,\n",
    "        \n",
    "    nll_loss = smooth_nll_loss / counter\n",
    "    nll_loss_x = smooth_nll_loss_x / counter\n",
    "    c_loss = smooth_c_loss / counter\n",
    "    c_loss_x = smooth_c_loss_x / counter\n",
    "    kl_loss = smooth_kl_loss / counter\n",
    "    total_loss = smooth_total_loss / counter\n",
    "    macro_f1 = smooth_macro_f1 / counter\n",
    "    micro_f1 = smooth_micro_f1 / counter\n",
    "       \n",
    "  \n",
    "    return model, train_metrics, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, total_loss, macro_f1, micro_f1\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "from sklearn.metrics import roc_auc_score\n",
    "test_temp_label = []\n",
    "test_temp_pred_x = []\n",
    "def test(model, test_loader, args):\n",
    "    counter = 0\n",
    "    model.eval()\n",
    "    # smooth means average. Every batch has a mean loss value w.r.t. different losses\n",
    "    smooth_nll_loss=0.0 # label encoder decoder cross entropy loss\n",
    "    smooth_nll_loss_x=0.0 # feature encoder decoder cross entropy loss\n",
    "    smooth_c_loss = 0.0 # label encoder decoder ranking loss\n",
    "    smooth_c_loss_x=0.0 # feature encoder decoder ranking loss\n",
    "    smooth_kl_loss = 0.0 # kl divergence\n",
    "    smooth_total_loss=0.0 # total loss\n",
    "    smooth_macro_f1 = 0.0 # macro_f1 score\n",
    "    smooth_micro_f1 = 0.0 # micro_f1 score\n",
    "    with torch.no_grad():\n",
    "     for _,data in tqdm(enumerate(test_loader),desc='Testing'):\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "      \n",
    "        output = model(targets,ids, mask, token_type_ids)\n",
    "\n",
    "        total_loss, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, cpc_loss, _, pred_x = \\\n",
    "                    compute_loss(targets, output, args)\n",
    "        \n",
    "        auROC = roc_auc_score(targets, pred_x)\n",
    "        test_metrics = evals.compute_metrics(pred_x.cpu().data.numpy(), targets.cpu().data.numpy(), 0.5, all_metrics=False)\n",
    "        macro_f1, micro_f1 = test_metrics['maF1'], test_metrics['miF1']\n",
    "       \n",
    "        smooth_nll_loss += nll_loss\n",
    "        smooth_nll_loss_x += nll_loss_x\n",
    "        smooth_c_loss += c_loss\n",
    "        smooth_c_loss_x += c_loss_x\n",
    "        smooth_kl_loss += kl_loss\n",
    "        smooth_total_loss += total_loss\n",
    "        smooth_macro_f1 += macro_f1\n",
    "        smooth_micro_f1 += micro_f1\n",
    "\n",
    "        counter+=1\n",
    "        \n",
    "    nll_loss = smooth_nll_loss / counter\n",
    "    nll_loss_x = smooth_nll_loss_x / counter\n",
    "    c_loss = smooth_c_loss / counter\n",
    "    c_loss_x = smooth_c_loss_x / counter\n",
    "    kl_loss = smooth_kl_loss / counter\n",
    "    total_loss = smooth_total_loss / counter\n",
    "    macro_f1 = smooth_macro_f1 / counter\n",
    "    micro_f1 = smooth_micro_f1 / counter\n",
    "          \n",
    "    return test_metrics, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, total_loss, macro_f1, micro_f1,auROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.a_max=0\n",
    "def experiment(model, train_loader, device, args):\n",
    "\n",
    "    for epoch in range(args.max_epoch):\n",
    "        model.train()\n",
    "        model, train_metrics, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, total_loss, macro_f1, micro_f1= train(model, train_loader, args)\n",
    "        train_acc=train_metrics['ACC']\n",
    "        train_ha_acc=train_metrics['HA']\n",
    "        print('- Epoch :', epoch+1)\n",
    "        print('*** Training Metrics ***')\n",
    "        print('- NLL Loss : %.5f' % nll_loss,'- Total Loss : %.5f' % total_loss, '- Total Accuracy : %.5f',train_acc,'- Hamming Accuracy : %.3f',train_ha_acc,'- Micro F1 : %.3f',train_metrics['miF1'], '- Macro F1 : %.3f',train_metrics['maF1'])\n",
    "        \n",
    "        train_losses[\"nll_loss\"].append(nll_loss)\n",
    "        train_losses[\"total_loss\"].append(total_loss)\n",
    "        train_losses[\"acc\"].append(train_acc)\n",
    "        train_losses[\"hamm_acc\"].append(train_ha_acc)\n",
    "        train_losses[\"macro_f1\"].append(train_metrics['miF1'])\n",
    "        train_losses[\"micro_f1\"].append(train_metrics['maF1'])\n",
    "        train_losses[\"auc\"].append(train_metrics['meanAUC'])\n",
    "       \n",
    "        \n",
    "        # Validation\n",
    "        test_metrics, nll_loss, nll_loss_x, c_loss, c_loss_x, kl_loss, total_loss, macro_f1, micro_f1,aucROC=test(model,test_data_loader,args)\n",
    "        test_acc=test_metrics['ACC']\n",
    "        test_ha_acc=test_metrics['HA']\n",
    "        print('- Epoch :', epoch+1)\n",
    "        print('*** Validation Metrics ***')\n",
    "        print('- NLL Loss : %.5f' % nll_loss,'- Total Loss : %.5f' % total_loss, '- Total Accuracy : %.5f',test_acc,'- Hamming Accuracy : %.3f',test_ha_acc,'- Micro F1 : %.3f',test_metrics['miF1'], '- Macro F1 : %.3f',test_metrics['maF1'])\n",
    "        print(\"AUCROC\",aucROC)\n",
    "\n",
    "        args.a_max=max(args.a_max,aucROC)\n",
    "\n",
    "        test_losses[\"nll_loss\"].append(nll_loss)\n",
    "        test_losses[\"total_loss\"].append(total_loss)\n",
    "        test_losses[\"acc\"].append(test_acc)\n",
    "        test_losses[\"hamm_acc\"].append(test_ha_acc)\n",
    "        test_losses[\"macro_f1\"].append(test_metrics['miF1'])\n",
    "        test_losses[\"micro_f1\"].append(test_metrics['maF1'])\n",
    "        test_losses[\"auc\"].append(test_metrics['meanAUC'])\n",
    "        \n",
    "     \n",
    "\n",
    "        args.scheduler.step()\n",
    "        torch.save(model, f'model_{args.dataset}_{args.max_epoch}.pt')        \n",
    "        torch.save(train_losses, f'train_metrics_{args.dataset}_{args.max_epoch}.pt')\n",
    "        torch.save(test_losses, f'test_metrics_{args.dataset}_{args.max_epoch}.pt')\n",
    "   \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the train model here\n",
    "from tqdm import tqdm\n",
    "args = experiment(model, train_data_loader, device, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i.item() for i in test_losses['hamm_acc']])\n",
    "plt.title(\"Hamming Accuracy on Tox-Cast Test Set\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a figure with one row and four columns\n",
    "fig, axes = plt.subplots(1, 6, figsize=(12, 4))  # Adjust figsize as needed\n",
    "train_losses=torch.load('train_metrics_tox21_200.pt')\n",
    "\n",
    "\n",
    "# Plot each list in a separate subplot\n",
    "axes[0].plot([i.item() for i in train_losses['nll_loss']])\n",
    "axes[0].set_title('NLL LOSS')\n",
    "\n",
    "axes[1].plot([i.item() for i in  train_losses['acc']])\n",
    "axes[1].set_title('TOTAL ACCURACY')\n",
    "\n",
    "axes[2].plot([i.item() for i in train_losses['hamm_acc']])\n",
    "axes[2].set_title('HAMMING ACCURACY')\n",
    "\n",
    "axes[3].plot([ i.item() for i in train_losses['macro_f1']])\n",
    "axes[3].set_title('MACRO F1')\n",
    "\n",
    "axes[4].plot([ i.item() for i in train_losses['micro_f1']])\n",
    "axes[4].set_title('MICRO F1')\n",
    "\n",
    "axes[5].plot([ i.item() for i in train_losses['total_loss']])\n",
    "axes[5].set_title('TOTAL LOSS')\n",
    "\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
